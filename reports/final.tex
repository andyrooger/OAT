\documentclass{report}
\usepackage{amssymb,amsmath}
\usepackage[mathletters]{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[breaklinks=true,unicode=true,pdfborder={0 0 0},colorlinks=false]{hyperref}
\usepackage{listings}
\lstdefinelanguage{obftool}{morekeywords={help,quit,status,parse,explore,format}}
\lstset{language=Python, numbers=left, showstringspaces=false, frame=single}
\usepackage{pst-gantt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setcounter{secnumdepth}{0}

% For title page
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}
\begin{titlepage}

\begin{center}

% Upper part of the page
\textsc{\LARGE Imperial College London}\\[1.5cm]
\textsc{\Large Undergraduate Individual Project}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{\huge \bfseries Obfuscating Python 3000} \\[0.4cm]
\HRule \\[0.4cm]

Code on git at: ssh://user@shell4.doc.ic.ac.uk/homes/asg08/git/ugproject.git \\[1.5cm]

% Author and supervisor
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Andy Gurden
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Herbert Wikliky
\end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large \today}

\end{center}
\end{titlepage}

\begin{abstract}
TODO write abstract!
\end{abstract}

\section*{Acknowledgements}

TODO - write acknowledements - Hayley Hannam, Herbert Wikliky, Chris Hankin, Alex Lamaison

\tableofcontents

\clearpage

\section{Introduction}

With the increasing popularity of interpreted languages like Java or Python, we are starting
to see a change in the way that program code is distributed.

Not so long ago, a programmer could write their program in a compiled language such as C. This would then be compiled
to machine code for a given architecture, and the program could be handed out to anyone without
worrying about whether a hacker or reverse-engineer could recover the original source code.
Of course, it can be done and there are tools to reverse the translation, called decompilers \cite{cdecomp}, but it
is a difficult process and there are methods to make this harder \cite{disres}.

The difference for interpreted languages is that much more of the original code is kept in the
distributed format. Java code, for example, is usually distributed as bytecode for the Java Virtual
Machine (JVM). This still holds information about high level constructs such as class and method names for example \cite{classinfo},
making it much easier to decompile a Java class.

In a language like Python it is usually raw source code that is distributed, therefore a
user need not put in any effort to view the inner workings of the program. While this is good
for the security of a user \cite{noobf}, it is often the case that the author will not want sensitive,
perhaps proprietary, parts of their code open to inspection or theft. With this in mind, we will work on ways to protect it.

We should note that there are existing solutions for the protection of Python source code, examples of which vary in quality.
These will be discussed in the next section, along with their drawbacks and areas to which we can contribute further.

The approach we will take to the above problem is to obfuscate the Python source code. This technique involves changing the
original program in some way so that it is more confusing to a reader or more difficult to analyse, while still remaining correct.

More specifically, during this project we shall implement a basic functioning, but extensible, obfuscator
that can modify the control flow of a program with the aim of diverting a human reader from its actual
function. This will act upon Python source code and should help to address the issue described above.
In the interest of evaluation, we will also create a sister program that will try to reverse the modifications performed by the
obfuscator with the aim of recovering the original source code. 
We shall aim to create these tools by taking a subset of obfuscation techniques used on other languages (e.g.
loop flattening or instruction reordering \cite{taxobftrans}) and looking at how well these will transfer to such a dynamic language as Python.

On the surface this may appear rather simple, however as Python is such a highly dynamic language it can be very difficult to analyse
\cite[p13]{staticanal} as thoroughly as a more traditional compiled language such as Java. In such languages, it is much easier to perform a static
analysis of the code. This is simply not feasible for many applications in Python, yet many program transformations will require this type of
thorough analysis. Issues in this area may appear or they may not, depending on the techniques we decide to implement, and it will be a part of
this project to notice and evaluate these issues.

\section{Background}

Here we will discuss Python and some solutions for protecting code, as well as some obfuscation techniques
and current tools.

\subsection{Python}

As we have discussed in the introduction, the project aims to implement known obfuscation techniques on Python source.
The language is used widely, from the popular web framework Django \cite{django} through many applications
from Google \cite{pygoogle} and even embedded as a scripting language for extensions in programs like GNU
Image Manipulation Program (GIMP) \cite{gimp}. Due to its wide and varied use, it is of increased importance
that someone wishing to protect their Python code from theft or reverse-engineering can do so.

To be specific about the code and techniques, we will restrict our efforts to using obfuscation methods that will
result in standard Python code so that it is portable across any of the many Python runtime implementations. The version
of Python we will be working with is 3.x (nicknamed 3000 or py3k) for both source and generated code. The most popular
version of Python is still 2.x, however this version will soon ``become stale'' as the language moves towards 3.x, on its way
dropping some compatibility with 2.x.

\subsubsection{Methods of Protection}

There are a number of ways Python developers currently try to hide or protect their code, with varying degrees
of success and security.

One method is to use a Python tool called Freeze. This tool will simply take given source code and compile
it to bytecode. This will be combined with necessary parts of the runtime to allow a program to run
on systems without Python installed. As it says in the README \cite{freezereadme} this provides little if any
protection as Python's standard library comes with a disassembler ready to view the bytecode. There are also
tools such as AntiFreeze \cite{pirates} to help analyse the code, as well as a program called
decompyle \cite{decompyle} that will actually try to generate the original source for older bytecode ($\le$version
2.3). There is a similar tool to Freeze, called py2exe \cite{py2exe}, that will create a Windows executable but
this too suffers from the same problems.

From the above paragraph, it's easy to spot that just distributing the Python bytecode, another method occasionally
used, also fails to thwart reverse-engineering attempts. In fact there is another problem
with these methods and any others that rely on Python bytecode. Bytecode is an implementation detail \cite{dis}
of the default Python implementation, CPython. Relying on this to distribute code would glue any
user to a specific implementation of Python and possibly even to a specific version. This would remove the portability
expected from a Python program and contradict the condition that solutions must be portable, specified earlier.

A more secure solution for protecting Python code, and one that works for many people, is to remove the most
sensitive parts and replace them with C extensions to the Python program. This way the code becomes machine
code, it can use the wealth of obfuscation tools for C and being compiled machine code makes it much
harder to reverse engineer. Again though, this is based on CPython's ability to include C modules, restricting a
user to a particular runtime as well as a particular machine architecture (that the modules are compiled for).

A portable way to protect the code would be to obfuscate it, performing source to source translations that can
confuse a reader or program analyser. This is not and will never be a perfect solution, because all attempts
to avoid reverse engineering will be overcome eventually. The difference between one method of protection and
another is the time and effort taken to break it and how much the result is worth to the attacker. There are already
programs for obfuscating Python code, though the selection is limited. These will be discussed later.

\subsubsection{Ethics of Obfuscating}

Although it may not be obvious, there are ethical and possibly even legal implications to obfuscation.
Specifically for Python, the language is based on a philosophy of clear and readable code, and actually
enforces this by the design of the grammar. Obviously, obfuscation is an attempt to take this away.

This hits out against certain expectations a user may have about a Python program. A key benefit of having your
software handed to you in source form is that you always know what it's doing. Deliberately obscuring the
function of software, but in a format that is almost always open for inspection, removes trust. The vendor or developer is
denying the user trust by not allowing them to use the software as they wish, but more importantly the user's
trust in the software can be lost.

If the software is hiding what it is doing this could be for a legitimate reason, such as hiding novel ideas
from competitors, or it could be to mask malicious content. A user or program, provided the obfuscation is
good enough, cannot tell which the reason is. This is a technique often employed by malware
writers to avoid detection by anti-virus software \cite{dycodeobf}. In fact, it has been argued that any and
all obfuscated code should be treated as if it were malware \cite{noobf}, assuming software is guilty until
proven otherwise.

My view and the view we will take for the remainder of the paper is that these issues create more reason for
building a tool such as this. By allowing issues such as those described above to block research into a subject
we are making ourselves much more vulnerable to those who would abuse the technology. Those who may abuse any
product of this project will be free to do so, as will those who would use the tools and information legitimately.

It is important to note that although my conclusion on this subject would be the same were I based outside of
the UK, the legal implications stemming from it may not be. For example in German law, clause 202(c) \cite{202c}
could make it illegal to write or distribute this software if it could be used for
certain forms of cybercrime. As discussed above, it probably could, and would have implications for anyone distributing
or using the information presented here.

\subsection{Types of Obfuscation}

Having decided to obfuscate the code from source to source, it is worth taking a look at the types of
obfuscation already used, how they are useful and which we will pursue. We will classify them into 3 categories
suggested by \cite[p10]{desevobf}.

\subsubsection{Layout Obfuscation}

These will apply transformations to the source language or possibly bytecode that do not affect the running of the
program. This is a very common form of obfuscation \cite[p10]{desevobf}, and involves transformations such as removing comments,
scrambling identifiers, or removing as much whitespace as possible to make the code unreadable.

Comments and identifiers often hold a lot of semantic information as they are designed to do.
Removal of this information by deletion or renaming is quite effective and almost always irreversible. Other transformations involving syntax
can be easily removed by a source formatter and so will only be effective against an impatient human.

While some of these transformations may be easy to perform in traditional languages, there are places in which
Python's constructs can cause problems. Fortunately these kinds of transformations are already covered in existing
tools, although these are not necessarily compatible with Python 3. For these reasons, we will be focusing on other forms of
obfuscation which have received less attention.

\subsubsection{Data Obfuscation}

This type of obfuscation transforms data layout and can help to obfuscate the structure of the program. It can be
particularly helpful in Python as the language makes it very easy to inspect programs as they run, dissecting data
structures and learning about how the program works.

We can look towards Collberg 1998 \cite{dataobf} to see some different types of data obfuscation. These are summarised here:

\begin{description}
\item[Classes] \hfill \\
Classes are a great tool for data abstraction and can often tell us a large amount about the organisation of a program. This is
especially important in an object-oriented language such as Java, but is also true for Python.

To break this information we have a number of options, often used together. The Chidamber metric says a class becomes more complex,
the further it resides down the inheritance tree. This means we can complicate the program by increasing this depth, either by adding
false classes along the tree, or by splitting classes into two or more that inherit from each other.

We could also complicate the structure by using 'false refactoring', which involves merging unrelated classes to form a new parent
class.

\item[Arrays] \hfill \\
Often program code will iterate over an array. There are a number of ways we can alter the structure of the arrays to complicate
this.

A single array can be folded or flattened, this increases or decreases its dimensions. We can also split one array into multiple
arrays, or merge multiple arrays into a single one.

\item[Functions] \hfill \\
These can be obfuscated by applying similar techniques as for classes. Methods can be combined by inlining, or split by outlining.
We can also clone methods, duplicating the original function so to make it hard for a reverse engineer to see that the same
functionality is being used in different places.

We could also use table interpretation, converting the contents of the function into a virtual machine code and using an internal
interpreter to run it when the function is called.

\item[Basic Types] \hfill \\
We can replace basic types such as strings with methods that produce them. As with most other structures we have looked at, we can
also merge or split variables. For example a boolean variable may be split into multiple boolean values that are combined with a
logical expression.

An example from Collberg describes merging multiple integers into one single integer by using the upper bits for one and the lower
bits for the other. This technique can also work in Python, but as integer types are of unlimited precision \cite{intprec} in Python
3.x we need to make sure we can bound the values an integer can take at any point during our program.

\end{description}

While this type of obfuscation can be useful, our focus will be primarily on the next type of obfuscation.

\subsubsection{Control-flow Obfuscation}

Control flow obfuscation alters or obscures the control flow of the original code. This will confuse an analyser as to the
true control flow of the program.

This time we can look to Collberg 1997 \cite{taxobftrans} for examples of control flow obfuscations. Many of these will use the idea
of an opaque predicate, a variable whose value is known by the obfuscator but hard for a deobfuscator to calculate.

By depending on the value of an opaque predicate, a transformation can be made more resilient against a deobfuscator. As the
deobfuscator finds it difficult to calculate the value of this predicate, it cannot reduce the tranformation or possibly
remove it.

We will see this used in the following types of control-flow obfuscaton:

\begin{description}
\item[Inserting Dead Code] \hfill \\
Here we branch the code at some point during a sequence of statements based on some opaque predicate. The branch will either
direct the program flow to the rest of the statements, or to some other code of our choosing.

If we know the value of the opaque predicate we can make sure that the program still follows the same flow, otherwise we
ensure both branches contain the same code or at least code that performs the same action.

We can disguise this branch by making sure any dead code we insert is similar to but possibly a broken version of our real code.

\item[Modifying Loop Conditions] \hfill \\
This is suggested in Collberg in reference to the Java programming language. The idea is to extend the loop condition using a
complex opaque predicate that will not affect the truth value of the condition.

This will translate to a \texttt{while} loop in Python, however it will not work inside a \texttt{for} loop. This is due to
Python \texttt{for} loops iterating over some iterable data type, rather than the familiar C-like for loop that iterates
through indices until a condition turns false.

We could try to apply a similar transformation to Python \texttt{for} loops by inserting or appending one or more special values
into our iterable object. Inside the loop we would need to make sure any of these special values are ignored, but use further
obfuscation to ensure the modification is well-hidden.

\item[Removing Idioms and Library Calls] \hfill \\
The benefit of doing this is obvious. These are both common parts of program source code that a reverse-engineer can look at
to gain their bearing. Common idioms will indicate the intention of a section of code much more strongly than other arbitrary
code.

Likewise library calls, especially in Python, are usually very well documented. This documentation gives the reverse enginner
a great deal of information relating to the values of variables around the call. The danger of these library calls can be
mitigated somewhat by obfuscating the names of imported libraries, but it still may be better to replace them with calls
to undocumented custom libraries.

\item[Table Translation and Inlining or Outlining Functions] \hfill \\
These methods have already been described in the data obfuscation section. While they obfuscate the structure of a procedure,
they also obfuscate the control flow inside of it so we will include them here too.

\item[Adding Redundant Operations] \hfill \\
This applies to any expressions where we may add an operation that does nothing. For instance, any integer expression may
be multiplied by $1$ or added to $0$, and any boolean expression may be replaced by its conjunction with \texttt{True}.

Of course rather than using simple integers or boolean values, we can complicate the expressions by using opaque predicates
that we know evaluate to these values.

\item[Parallelising Code] \hfill \\
If we can be sure we won't create any of the issues faced by concurrent programmers, we can take two independent parts of
the code and parallelise them. Alternatively we could introduce an entirely new process to run along side our current code.

When they run concurrently, it is difficult for an analyser to split these again. This could be made even more complicated
by introducing one or more shared variables whose value we do not care about between the processes.

\item[Program Locality] \hfill \\
A programmer will usually write code in which logically related parts of the program are situated close together. This information
can be used by a reverse-engineer, so our task it to try and randomise the location of each component of the program.

Collberg makes a distinction here between the level of a program we are looking at. The reason being that, in Java, moving components of
a program is very different depending on whether you are moving classes between files, methods inside classes, or statements around
inside a method.

As we are working in Python we still will need to make a distinction if we want to move components between different files. Other
than this, most levels of the program act very similarly. At module level, class level or function level you can run code, define
classes or functions, or import other modules. The only difference is how that level interacts with the levels below it.

\end{description}

As we can see, there are a broad range of different possibilities when it comes to control-flow obfuscation. Reversal of this type
of transformation can require careful analysis, and can be difficult to perform. This would involve analysing the program to recognise
opaque predicates and variable dependencies. It is very possible that the same code used to reverse a control-flow obfuscation will
actually optimise the program it is deobfuscating.

These are the type of transformations we will attempt to implement first, due to the sheer range of options to choose from.

\subsection{Existing Tools}

We have previously seen that there are tools to do similar obfuscation in Python already. We will now discuss some
examples of these, as well as other tools to try to reverse the transformations.

\subsubsection{Python Obfuscation}

There are a number of tools out there that claim to do this. For example BitBoost Systems have a Python obfuscator \cite{bitboost}
that claims to use layout obfuscation as well as ``psychologically inspired techniques'' to confuse readers. Sadly, as a
single machine license costs \$200 it is not possible to test the tool outside of their web-based demo.

In the free realm, the freeze \cite{freezereadme} or py2exe \cite{py2exe} programs will also obfuscate python very slightly,
though only by distributing bytecode rather than raw source.

Alternatively pyobfuscate \cite{pyobf} will scramble a subset of the identifiers used in programs as well as performing
some other layout transformations. It has its limitations however \cite{pyobf}, so is not powerful enough for real use cases.

pyobfuscate has not been updated since 2005, and is unable to run on any Python version since 2.6. Many of the other tools are in
a similar situation and so it is unlikely these will be able to cope with recent versions of Python code.

\subsubsection{Analytic Tools}

For analysis of Python code there are more tools available.

Firstly to reverse the very basic syntax transformations there are a great number of Python pretty printers such as
pygments \cite{pygments} or PythonTidy \cite{pythontidy}. This should never be a particularly difficult task as Python is
designed with readability in mind and enforces clear formatting in its syntax.

PyLint \cite{pylint}, PyChecker \cite{pychecker} and PyFlakes \cite{pyflakes} are all tools designed to help look for possible
bugs in Python code and so may have some use in checking validity of obfuscated source files. They also look for bad design and so
could help to determine how difficult a program is to understand for a human reader.

PyDev \cite{pydev} is a plugin for the Eclipse IDE for developing in Python. It performs some useful code analysis on projects to detect
possible bugs and allow easy refactoring of code. While it is implemented in Java as well as Jython (an alternate Python runtime)
it should be possible to reuse the ideas if needed. Tools for refactoring are easily available as PyDev uses Bicycle Repair
Man \cite{bikerepair}, a Python library designed specifically for this task. PyDev also definitely supports Python 3.x as opposed to the
other tools in which this is unclear.

To attempt decompilation of the Python bytecode if necessary, there is a tool called decompyle \cite{decompyle} that has already been
mentioned. This should not be necessary though as we have already discussed reasons not to use bytecode during the project.

Some of the greatest  tools for analysis come from Python itself. Programs are often run from an interactive Python interpreter, and this
can be used for easy dissection. Python's standard library comes with modules for parsing, assembling and disassembling source code.
There are also tools for creating interactive sessions within the program. For example by inserting the following code at any point
in the program:

\lstinputlisting{../snippets/breakpoint.py}

We can effectively create a breakpoint and launch an interactive shell to inspect and possibly modify the current local
variables. The program will continue as soon as the shell is closed.

Other more sophisticated tools are available for debugging or inspection such as AntiFreeze \cite{pirates} if necessary,
however Python provides more than enough in its standard library.

\section{Creating the Software}

The end product of this project is intended to be a usable source to source Python obfuscator as well as an analytic tool
to assess its effectiveness. If we are to make sure the transformations we make to programs are really correct, we should
know the ins and outs of the language well. This way we can easily pick up on language features that may contradict our
expectations about the behaviour of the code. For example it's easy to naively assume that a simple assignment to an object
variable would be free of side effects. This is not the case, depending on the actual code an assignment can run absolutely
anything \cite{pyprop} and doesn't even have to assign the given value to a variable.

To learn the language to the desired degree we will also use it to write our software. The hope being that heavy use will help to
drill out some of the less obvious features. For the same reason, we will be writing the code specifically to run on a Python version 3.x
interpreter.

Having chosen the language and version we will use to develop in, we need to design the software we are about to write. This is split into a few basic building
blocks. Specifically the following:

\begin{itemize}
\item A Python parser/lexer to read and make sense of Python source files.
\item Implementations of a number of obfuscating transformations.
\item Implementations of a number of analytic techniques.
\item A Python writer to take our abstract representation of the source code and output a file.
\item A user interface.
\end{itemize}

The most fundamental parts here in the workflow are the parser/lexer and the writers for the source files.
Without these the other parts are useless, so this is where the implementation will begin.

\subsection{Reading and Writing Python Source Code}

It is easy to assume that reading and writing Python source code are two discrete problems. In fact this is not the case,
when looking at most possible solutions we shall see that the method or tools used to read the source code are usually deeply
entwined with each other. This will either be because they are part of the same toolset, or just because they work so closely together;
any data structures output by one need to be readable by the other. For this reason we shall tackle these problems as one.

There are plenty of free tools that read and write source as part of their tasks. For example PyDev, mentioned in the background section, will
perform these tasks during code formatting. For this project we have the choice between writing software from scratch, attempting
to recycle relevant parts from free projects, or using libraries to take the work away from ourselves.

The first option we should look at is to write the code from scratch. This could be very time consuming, requiring
us to write complicated software as well as studying documentation on Python's grammar which is freely available online \cite{pygrammar}.
While the language is well documented it does seem wasteful to repeat work that has been done in so many other places.

Next we look at extracting the functionality from other free (open source) software. The most fitting source to source translation tools
we could use for this are PyDev or the 2to3 tool \cite{2to3}, used to convert Python 2.x source code to Python 3.x. Ultimately we will not
delve into these tools due to the amount of time we could squander searching for relevant parts of code. It is likely we
would come to the conclusion that they outsource most of the work to separate libraries anyway.

This leaves us using libraries. In fact the 2to3 documentation points to a Python standard library called \texttt{lib2to3} that can be used to
perform automatic translation on Python source. Unfortunately further reading shows that the library API is unstable and could change by
the next release. This is not a situation we wish to to deal with and so is not really a viable option.

Fortunately the Python standard library contains a number of modules devoted to its own language. By using these to parse a source file
we can hold the source as an abstract syntax tree or AST. This will give us the freedom to transform the language as we like and the only code we
will need to write would be for the AST translations and an AST to source code writer.

There are some flaws to storing the program as an AST, as this structure drops certain information.
For example the \texttt{elif} statement, meaning the same as an \texttt{else}-\texttt{if} statement in other languages allows many tests without
additional levels of nesting in source. When this construct is parsed it becomes an \texttt{if} statement within an \texttt{else} statement and
can create large changes to the expected code and level of nesting. The example below shows the result of translating the code on the left hand
side to an AST and back, writing the result on the right.

\begin{minipage}[b]{0.4\linewidth}
\centering
\lstinputlisting[basicstyle=\small]{../snippets/nest.py.before}
\end{minipage}
\hspace{1cm}
\begin{minipage}[b]{0.5\linewidth}
\centering
\lstinputlisting[basicstyle=\small]{../snippets/nest.py.after}
\end{minipage}

Information such as comments are also dropped from the representation. These seem to be reasonable compromises however and so
using Python's \texttt{ast} module and writing our own AST to source writer is the option we will use. It gives a lot of freedom, whilst
being fast to write.

Before we start to design our source writers, we should think about the data structures we will be using to store information. As we don't have
control of the \texttt{ast} module, we cannot decide what format will be output when parsing a file. In fact, the value we should receieve from
a successfully parsed file will be derived from the \texttt{ast.AST} class. More specifically it will be a child of \texttt{ast.mod}, either
\texttt{ast.Module}, \texttt{ast.Interactive}, \texttt{ast.Expression} or \texttt{ast.Suite} depending on the context in which we are parsing.

It may be tempting to use this format throughout our software, however it does have its drawbacks. We will take a look at the design first to see why.

An object of any class derived from \texttt{ast.AST} represents a node in the AST grammar given in the documentation for the \texttt{ast} module. The
type of the node is the name of the class it belongs to. Each of these nodes can have a number of attributes or children, these are either of basic
types such as strings and integers or they are single or lists of other \texttt{ast.AST} nodes.

For most types of node the names of these children vary so it is useful to be able to find out their names from the object we are inspecting. For this we
look at the \texttt{\_fields} attribute, which is a tuple of all fields names. Using this we can look through the whole tree without worrying about the type
of individual nodes. There is also an \texttt{\_attributes} attribute giving the names of attributes. These are \texttt{lineno} and \texttt{col\_offset} where
available. We access field or attributes as normal attributes of the class.

This is a nice system and will help with tasks such as exploring or displaying the tree, however it creates problems as it is not stored as a tree of the same
type. Instead it is stored as a tree of \texttt{ast.AST} nodes, lists, and basic types. Unfortunately this means any tree traversal will have to be designed to
cope with either having or not having a proper \texttt{ast.AST} node and cannot rely on being able to find or read a \texttt{\_fields} attribute. Additionally we
cannot add new attributes onto lists, and we see later that we need to do this for any node.

\subsubsection{Implementing Internal Data Structures}

As there are a lot of positives for using \texttt{ast.AST} nodes as our data structure, and as we are forced at least to begin with one of these nodes after parsing,
it does not make sense to start again from scratch. Instead our solution will be to create a wrapper class. This will hold our actual node, but provide a new set of
children - the wrapped version of each of our original children.

The wrapper class will be called \texttt{CustomAST} and provide a number of helpful methods:

\begin{description}
\item[\texttt{\_\_init\_\_(node)}] \hfill \\
Stores the given original \texttt{ast.AST} (or basic or list) type node in the attribute \texttt{\_node} and creates a dictionary mapping field names
to \texttt{CustomAST} children in the attribute \texttt{children}.

The dictionary is created by first taking string names for each of the node's children if they exist. For an \texttt{ast.AST} node this is easy.
Otherwise the node is a basic type and has no children, or the node is a list and we use string versions of indices into the list. Now we can
easily map these names to the children they refer to, and place the \texttt{CustomAST} versions of them into the dictionary.

We can also give a hybrid node to this constructor. This would be either a list containing normal nodes and \texttt{CustomAST} nodes, or an
\texttt{ast.AST} node with some \texttt{CustomAST} nodes as children. Children that are already \texttt{CustomAST} nodes will not be rewraped
and the given node will be altered to contain only normal nodes. This allows us to easily transform or create new nodes from \texttt{CustomAST}
nodes without dissecting them first.

\item[\texttt{type(asclass)}] \hfill \\
Gives either the class of the node used to create this \texttt{CustomAST} node, or the string name of that class. This is helpful to easily distinguish
the node type.

\item[\texttt{node()}] \hfill \\
Returns the original node used during creation. This is in case we need to use a function designed for \texttt{ast.AST} nodes.

\item[\texttt{is\_ast()}, \texttt{is\_basic()} and \texttt{is\_list()}] \hfill \\
Allows checking of the broad category that the node falls into. These say, respectively, whether the nodes is derived from \texttt{ast.AST}, whether
it is a basic type such as a string or integer, or whether it is a list.

\item[\texttt{is\_empty()}] \hfill \\
Checks whether this node actually represents the lack of a node. If a parent node has an optional field, rather than just deleting field when the child does
not exist, the child will be set to \texttt{None}. This will then become an empty node. Likewise, if a parent has a field that can hold multiple children, an
empty list would represent that none exist and so this is also an empty node.

\item[\texttt{temp\_list(*vargs)}] \hfill \\
When translating or transforming nodes, it is often useful to concatenate lists to each other. New lists of single \texttt{CustomAST} nodes can easily be
created using hybrid nodes as so: \texttt{CustomAST([node1, node2, ...])}.

Actually concatenating list nodes is harder, so this function will create a new \texttt{CustomAST} list node starting with this node. Each argument will either be
concatenated or appended to the new node, depending on whether the argument is a list or a single node.

\item[\texttt{become(node)}] \hfill \\
When transforming an AST we may want to edit the \texttt{CustomAST} node in place. This will make sure any and all parents still reference the correct
object. This function will replace the node it wraps by that of the given node to do just that.

\item[\texttt{ordered\_children()}] \hfill \\
Often we will want to iterate over all children in a specific order. This is especially important for a list node where we want to iterate from 0 to 1.
Unfortunately, after filling our \texttt{children} dictionary with fields and nodes, we cannot simply iterate over the keys. The iteration order will
depend on the hashing function used internally by the dictionary.

Instead we use this function to generate a list of fields in the correct order. This is also used later to generate iterators for the node.

\item[\texttt{location()}, \texttt{desc()} and \texttt{\_\_str\_\_()}] \hfill \\
These provide a tuple with line number and column number, a text description of the node and a combination of both respectively.

By providing \texttt{\_\_str\_\_}, we allow conversion from node to string using \texttt{str(node)}. This means we can program in a much more
descriptive manner.

\item[\texttt{\_\_getitem\_\_(item)}, \texttt{\_\_iter\_\_()}, \texttt{\_\_contains(item)\_\_} and \texttt{\_\_len\_\_()}] \hfill \\
These functions all work towards a more natural interaction with node objects. We can access children like a dictionary using \texttt{node["field"]}, or
iterate using \texttt{for field in node}.

We also have a membership test using \texttt{field in item} and number of children using \texttt{len(node)}. These can be used together by Python to provide
a lot of other useful functionality.

\end{description}

Now we can parse Python source code and we have produced the data structures as well as the conversion methods needed to represent the output. The next
step is to provide a writer for our structure to enable us to write back to source code.

\subsubsection{Implementing an AST to Source Code Writer}

We shall now describe a solid design and extendable implementation for this. The design is fairly simple.

We begin with an abstract class called \texttt{SourceWriter}. Any code wishing to write an AST to source code can expect to be given a writer
class that looks similar to this, and any writer classes should be derived from this base. \texttt{SourceWriter} will define a number of functions
to be used by its derived writer classes. Among the more important are:

\begin{description}
\item[\texttt{\_\_init\_\_(topast, out)}] \hfill \\
The constructor. This takes the top node of an AST tree to translate to source and a file-like object we can write our output into.

\item[\texttt{write()}] \hfill \\
Translates and writes the whole of our given AST into our output file.

\item[\texttt{\_write(node)}] \hfill \\
Writes out the given node (and any necessary children). This actually checks the type of the node and dispatches the work to the appropriate function.
If an appropriate function is not found then this function will raise an exception as in means we either have an invalid node or an invalid writer class.
The writing of any node should be done through here rather than the method for a specific node type.

\item[\texttt{\_ground\_write(s)}] \hfill \\
Writes the given string, \texttt{s}, to our file. Any function that wishes to write to file should be doing it through this function.
This allows added functionality such as being able to monitor the position in the file we are currently writing to.

\item[\texttt{\_inc\_indent(by)} and \texttt{\_dec\_indent()}]
These increase the indentation level by adding the specified string to the current whitespace, or decrease it by removing the most recent addition.
By giving a string rather than an indentation level we can choose between tabs or a certain number of spaces. We can even indent by strings such
as \texttt{">>>"} to indicate an interactive session.

\item[\texttt{\_write\_block(statements, indent)}]
Writes an entire block, consisting of the given statements. \texttt{indent} is a boolean value, so we can have a uniform method to choose how to
indent the block inside the function. This can be overridden by derived classes.

\item[\texttt{\_write\_\{classname\}(node)}]
These are all abstract methods that write specific node types for \texttt{\_write}. \texttt{classname} can be either the name of a basic type we
may want to write, or the name of an AST node class.

\end{description}

This design allows any node to be written recursively using the \texttt{\_write} method. When called, it will dispatch the work to an appropriate method who
in turn will either dispatch each of its components back to \texttt{\_write} or, for leaf nodes or small details, write itself using
\texttt{\_ground\_write}.

By using uniform functions for writing structures such as blocks, we allow a derived class to alter behaviour easily from
a single point. The benefit of doing so is that we make the source writer easily extendable to create pretty printers,
minifiers\footnote{Minification of a program involves shrinking its file size by removing as many unnecessary characters as possible. It is
often used for interpreted languages such as JavaScript that need to be lightweight for transmission.} or even, at a later date if we choose
to extend our implementation, a basic layout obfuscator.

Now as \texttt{SourceWriter} is an abstract class, we need to provide an implementation for each of its abstract methods. Our first concrete class
will be called \texttt{BasicWriter}. This should produce correct code but we will invest no effort in readability. By using \texttt{BasicWriter} as
a starting point, other classes can focus on layout or clever modifications without the need to worry about trivial parts of the implementation.

We will not discuss the implementation of \texttt{BasicWriter} as it is fairly simple. Each node is written with the bare minimum effort required
and there is no exciting processing that takes place. The only item of interest here would be to note that not all nodes are actually implemented;
there is a small group of nodes that represent context rather than content, such as whether a name is being loaded or assigned to. As these should
never be written, they instead raise a \texttt{NotImplementedError}.

As we will see shortly, the \texttt{BasicWriter} does not produce very palatable code. Therefore it is in our and our users interests to
create a writer with a greater focus on readability. This writer will be named \texttt{PrettyWriter} and can be used to ease the inspection of
translated code. As suggested previously it will be derived from \texttt{BasicWriter}, avoiding most of the work of creating a new writer.

The specific extensions we will include in \texttt{PrettyWriter} alter the output for only a handful of nodes:

\begin{description}
\item[\texttt{Expr} and \texttt{Str}] \hfill \\
An \texttt{Expr} node is a type of Python statement, this means it stands on its own line. If the \texttt{Expr} node contains only a \texttt{Str} node
(representing a string), it is known as a docstring. These are used for programatically accessible documentation and are usually represented
differently to normal strings.

By making the distinction between a string and a docstring we can ensure we recreate this difference in the written code. As the docstring conventions 
\cite{docstr} recommend, we use triple quotes and so multiline strings can actually be written on multiple lines. We also change some of the strings to
cope with differing indentation.

\item[\texttt{FunctionDef}] \hfill \\
Often in a function definition we need to specify large number of parameters. Each of these parameters can all be given default values, and Python 3.x
even gives us function annotations so we can add a comment after the name and default value of each parameter. \texttt{BasicWriter} will write all
of this on one line, which very quickly becomes unreadable.

We aim to fix this by guessing when the argument list will be long, and if so, writing each of the arguments on a separate line. The way we actually look
for long argument lists is by checking each of the arguments for annotations. If any exist, we assume it to be a long list.

\item[\texttt{list}] \hfill \\
Lists usually represent a suite or a list of statements. If this is the case, \texttt{BasicWriter} will write the statements one after another, each on a
new line. This can be hard to read and is very different from a human reader who will usually separate logical blocks of code with blank lines.

\texttt{PrettyWriter} will try to infer these logical blocks from the type of the statements and insert the necessary blank lines to split them. For instance,
\texttt{import} statements are grouped together and class or function definitions are always in a group by themselves.
\end{description}

Using this this class as our writer in future should allow us to concentrate on the effects of specific obfuscations later. It will avoid clouding the evaluation
of the success of a technique with the minor obfuscations introduced by \texttt{BasicWriter}.

To illustrate the differences between the two solid implementations of \texttt{SourceWriter}, following is a sample program passed through our implementation of
\texttt{BasicWriter} and \texttt{PrettyWriter}. We can see a large difference between the source code returned by the two writers, it is obvious that
\texttt{PrettyWriter} produces cleaner and more readable code.

Sample Program

\lstinputlisting[basicstyle=\tiny]{../snippets/writer.py}

Output from \texttt{BasicWriter}

\lstinputlisting[basicstyle=\tiny]{../snippets/writer.py.basicformat}

Output from \texttt{PrettyWriter}

\lstinputlisting[basicstyle=\tiny]{../snippets/writer.py.prettyformat}

All of these writers will be held in the \texttt{writer} package.

\subsection{Interacting with the Software}

The next important part to think about is the how the user will interact with the software. Without some form of front end, the
user would need to write their own program each time they want to use our obfuscation code. Let us first discuss the type of
interaction we, or a user, would will need to have with the software.

\subsubsection{Interaction Method}

Current tools such as pyobfuscate or BitBoost's obfuscator are automated. In pyobfuscate, which is one of the more interactive
of the tools, user interaction and control is limited to not much more than the choice of file to obfuscate, and indentation style. This means
that while a user does have a little control over what happens, they really just insert a file and are returned an
unreadable one.

We want to be able to effectively evaluate what our tool is doing to the source code. For this we need to be able to intimately control
which parts of the program are affected, and by which obfuscations. This would be unfeasable to attempt with an automatic program,
even one with tweakable parameters, therefore it is not really an approach we can use.

Knowing we have to be able to direct and control transformations, there are a few different ideas of how. In general, these fit into
either solutions that involve us interactively looking through the AST and choosing how to transform or analyse certain
nodes, or those that let us insert tags into the source code that mark structures and techniques for obfuscation.

There are methods already used in other tools that we could employ to tag code. For example the \texttt{doctest} module in the standard
library uses docstrings (programmatically accessible comment strings) to specify unit tests for the code. Instead of unit tests,
we could place some form of markup into the comments. This would allow us to specify transformations in text form, but we should
remember to remove the docstrings at the end of the obfuscation.

For example we could try the following format to indicate a function should have all of its statements reordered:

\lstinputlisting{../snippets/tag.py.docstring}

Another possibility would be to use empty decorators to mark functions or classes for transformation. These are usually used to
transform the code in some way at run time, but could easily do nothing and be picked up as flags for the obfuscator. Using this
method we would need to specify the decorators elsewhere and could write the above examples as:

\lstinputlisting{../snippets/tag.py.decorator}

This would require a separate file, called obfuscate here, that defines the decorators. Also we would need to remember to
remove these decorators after obfuscation, just like in the docstring example.

The trouble with these methods is that transformations would be restricted to single nodes that support either docstrings or
decorators. By placing this constraint on any obfuscations we want to make, we would be rejecting a huge range of options.

We can also try to create our own system to tag source code or nodes in an AST. If we choose to tag source code, we need
to either make sure our solution is valid Python syntax and so will translate cleanly into an AST using the \texttt{ast}
module, or that we can strip any tags out before parsing and still be able to match them up afterwards.

It then makes much more sense, if we are tagging code, to tag the AST directly. This way we are free to modify \texttt{CustomAST}
nodes in whatever way we like. Parsing will still work as normal as we do not have the AST yet, and we can feel safe that our AST
to source writers will still work as we create them ourselves.

Actually if we were to enable a user to browse and tag an AST, we would already have already created an interactive explorer for
the tree. It seems unreasonable to expect a user to use software such as this to tag their tree and then have to feed this tagged
tree into a separate application to perform the obfuscation.

This leads us back to an earlier suggestion of of obfuscating directly from the explorer. This way
a user can merely browse to a node of interest and tell the software immediately to perform the transformations they want. Any
parameters needed can be set at this point, and the user will be able to see the effect of their command as soon as they have issued
it.

There is also the possibility that some obfuscations may involve multiple nodes or require more complicated parameters. To cope with
this we can provide the interface, but allow a different method of interaction to be defined for each type of obfuscation.

At this point we have decided on the method we will use to interact with our software. The interface is expected to allow a user to
explore an AST generated from the program source. For any obfuscation type implemented, we will define and implement a bespoke method
of interaction from the AST explorer.

As well as browsing and obfuscating the AST, we also need to be able to generate the AST and convert it back to source code afterwards.
For this reason we will also need to include some way of parsing and writing the file.

\subsubsection{Designing the User Interface}

These decisions bring a new question, one of how the explorer will look. The two main choices are a command line tool or a GUI. Again
Python contains tools to make both much easier. These come in the form of libraries such as the \texttt{cmd} library \cite{pycmd} for command
interpreters or \texttt{tkinter} \cite{pytkinter} for GUIs.

First, let us look at the \texttt{tkinter} library. This contains the standard Python interface to the Tk GUI toolkit. While being
a very simple and friendly toolkit to use, there is still a large amount to learn before starting to develop. As the developer is
not familiar with this library, this would add a moderate delay before beginning development work.

After the initial learning curve however, a GUI may present some significant advances over a command line interface. For a lot of interactive
tasks the graphical interaction is more intuitive for the user. This would also allow a more natural presentation of information. Data like
the AST tree structure could be displayed in a widget such as \texttt{Treeview}, designed specifically for this type of data. It is hard to
understand how a command interface could provide these natural displays.

Now we can discuss the benefits of using the \texttt{cmd} library. Again the developer is not familiar with this library, but unlike
\texttt{tkinter}, it is very simple to begin. Even at it's bare minimum functionality, the provided command console offers a help
function and support for autocompletion. All other functionality is easily addable by subclassing and implementing functions at various
hook points.

It is true that some information will be difficult to display naturally using only text, but our focus is on the obfuscation rather than
the interface. Time is restricted for this task so as long as this interface provides adequate access to the back end software, speed
of development is the main factor here.

For this reason we shall be creating a command interface to our back end. The style of the interface will be designed to imitate a
Unix style shell, with Unix style commands. This means that the large user base of Unix style operating systems will be already familiar
and find learning to use this tool much easier than it would be otherwise.

We should leave the option of having graphical components open. If we find later in the project that some operations or information would
be clearer in graphical environment, then we can still write commands to open a GUI for the task. The back end code should be completely
separate from the interface so it should be easy to replace the command line with a full GUI later if the time is available.

This command line interface will be the point of entry for users who want to obfuscate their code as well as those who want to analyse
already obfuscated code. For this reason, we shall name it simply 'Obfuscation and Analysis Tool', or oat. If we do find time later in
the project to develop a graphical interface, we can now call it 'Graphical Obfuscation and Analysis Tool', or goat.

\subsubsection{Implementing the User Interface}

Having decided on the tools to create the interface, as well as its style and tasks it is required to perform, we now need to begin creating it.

As described briefly above, we will be developing using the \texttt{cmd} library. This will mean our starting point is the \texttt{cmd.Cmd} class.
This class can be started using the \texttt{cmdloop} method and will proceed to act as like a normal shell with only the command 'help' available.

Obviously we will want to add other available commands to this, and we can do this by creating a subclass implementing methods called
\texttt{do\_\{command\}}. These can be documented for the help command by either implementing \texttt{help\_\{command\}}, or providing
\texttt{do\_\{command\}} with a docstring.

There is also a huge item missing from the default \texttt{cmd.Cmd} class. By default any command run will return a value which is passed to a
hook method called \texttt{postcmd}. If this method returns \texttt{True} then we exit. As the only method defined by default, 'help', does not
cause the program to exit, the only way to quit is to cause an exception!

Our first task in writing the interface is then to create \texttt{CommandUI}, a subclass of \texttt{cmd.Cmd} and provide a way to quit the program.
This is very easy and just involves implementing a \texttt{do\_quit} method that returns \texttt{True} and hence quits the program. For ease we will
also implement a \texttt{do\_EOF} method that does the same and allows the user to quit using Ctrl\^{}D.

One useful function often present when working on the command line is the ability to cancel a long running operation using Ctrl\^{}C. If we try this
currently with our \texttt{CommandUI}, we will cause a \texttt{KeyboardInterrupt} exception and kill the entire program. To fix this, we will
override the \texttt{cmdloop} method, enclosing the call in a \texttt{try-catch} block that catches any \texttt{KeyboardInterrupt} exceptions and
restarts the command loop.

We have the basic setup now, but we still need to provide commands for parsing, writing, analysing and obfuscating. It does not make sense to modify
our \texttt{CommandUI} class, creating \texttt{do\_\{command\}} methods every time we want to add a new command to the console. Instead we will write
a method, \texttt{add\_command}, that we can pass any \texttt{Command} object to add its functionality to the console.

Writing the \texttt{Command} class is now our new task. This will need to expose any functionality that the \texttt{CommandUI} class needs to use. The
most important methods are listed below.

\begin{description}
\item[\texttt{\_\_init\_\_(id)}] \hfill \\
This gives the command a name. It will be used by \texttt{CommandUI} as the name to type when running the command.

As this is a constructor, it also initialises an \texttt{argparse.ArgumentParser} object which is used by subclasses to parse their arguments and
provide standard help messages. By expecting all commands to use an argument parser of this type, we are attempting to help standardise the user
experience.

\item[\texttt{do(line)}] \hfill \\
When a command is run from \texttt{CommandUI} it will, by default try to call \texttt{do\_\{command\}}. We override that functionality to search
through the list of added commands and run this method instead on the appropriate object.

This method will try to parse the arguments given on the command line. If successful, it will feed the result through to the \texttt{run} method,
otherwise it will print the help message and return.

\item[\texttt{complete(text, line, begidx, endidx)}] \hfill \\
If the user presses the tab key while typing, the \texttt{cmd.Cmd} will try to autocomplete the command by calling a specific set of functions. We
override this behaviour in \texttt{CommandUI} to outsource the work to a specific \texttt{Command} object, calling the \texttt{complete} method.

When this function is called, it is passed information that can be used to calculate possible completions of the line. At this point this information
is difficult to use and not very helpful to us. The information we should be returning is also non-intuitive.

To make this easier when programming later we translate this information into three strings; all text typed before the current word, all text after,
and everything that has been typed of the current word. This is then handed to the \texttt{autocomplete} method from which we
expect to receive a number of words that the current word could become. This is then translated back into the format needed by \texttt{cmd.Cmd} to
return and complete the loop.

\item[\texttt{help()}] \hfill \\
As suggested by the name, this method will print a help message from the command. This is generated using the argument parser initialised inside
the constructor. This will be called by the overridden help method in \texttt{CommandUI}.

\item[\texttt{status()}] \hfill \\
Another method we will add to \texttt{CommandUI} is \texttt{do\_status}, allowing us to use the command 'status'. This will be built into the
class rather than added with \texttt{add\_command} so that it has access to all of the other commands. It may then iterate over each and call
this status method to display the status of each command in turn. We expect this method to be overridden in an actual \texttt{Command} class.

\item[\texttt{run(args)}, \texttt{autocomplete(before, arg, after)}] \hfill \\
These are both designed to be overridden when implementing a concrete command, along with \texttt{status}. \texttt{run} is called to run the
command as described in \texttt{do}, and \texttt{autocomplete} is called to generate autocompletions as described in \texttt{complete}.
\end{description}

Now we know how to create and plug in the commands, we can start to create them. The initial commands we will implement are described below:

\paragraph{help}

This will list help for the program as a whole as well as individual commands. It is an essential tool for a user to find their way around a
command based program such as this one.

Most of this command is implemented by \texttt{cmd.Cmd}, but we have extended the functionality to forward the calls for individual commands
to the \texttt{Command} objects.

\paragraph{quit}

Used to exit the program as described earlier. This can also be achieved by using Ctrl\^{}D.

\paragraph{status}

Again this has already been described. When run, it will send a request to all added \texttt{Command} objects asking them to print
their statuses if they have anything useful to contribute. A user can use this command to check the state of certain parts of the
program.

\paragraph{parse}

Actual parsing of Python source files is done using this command, as well as loading and saving of the resulting ASTs. Most of the processing
is performed inside of the \texttt{ASTStorage} class. This class can be initialised, given a file name, by either parsing the source file, or
loading a saved version.

Parsing is performed by the Python standard library, \texttt{ast}, and then transformed into a \texttt{CustomAST} node as described earlier.
If we choose to load the AST, the \texttt{pickle} library is used to load the \texttt{CustomAST} node tree directly from the .ast file
corresponding to the Python source file. This is the same library used to save the AST in the first place.

We also record information to identify changes to the tree. When we save or load a file, we record with it a SHA-224 hash of the file. This
is used to ensure that we are loading the correct tree when we request the file later. As well as this information, we keep a \texttt{modified}
flag that tells us if the tree structure has been changed, and an \texttt{augmented} flag that lets us know whether any other information has
been added to the tree.

\paragraph{explore}

This is used to explore an AST generated by the parse command. Whenever there is a parsed tree, this command will point to a specific node within it.

By typing this command with no arguments we can view a description of the node we are looking at, as well as any children it contains. We can move
our pointer to a child node by using the child's name as an argument, or to the parent node by using the argument '--parent'.

It is also possible to view information such as the node's line number and column in the source by adding the '--attributes' flag.

\paragraph{format}

The integration point for the source writers. This command can print either the whole AST using the flag '--top' or the subtree pointed
to by explore by default. The printing style can also be changed by choosing to use \texttt{BasicWriter} or \texttt{PrettyWriter} with
the argument '--style \{basic, pretty\}'.

If the user chooses, output from this command may be written to disk using the flag, '--write FILE'.

Other commands will be added to this list as we develop the functionality to make them necessary, and all of the commands implemented
should employ autocompletion wherever possible. This helps to guide the user and speeds up the use of oat.

Now we can look at an example session, to see how the user will interact with the tool:

\lstinputlisting[language=obftool,basicstyle=\small]{../snippets/commandsession.log}

At this point we have created an interface, and we are able to parse, store, inspect and write Python source
files. It is time to begin designing the code that oat will use to obfuscate these programs.

\section{Developing Obfuscating Transformations}

As mentioned in the background research there are 3 main types of obfuscation; layout obfuscation, data
obfuscation and control-flow obfuscation. Of these we will focus on one, control-flow, although we shall
first digress to describe the layout obfuscation that we inadvertently perform.

\subsection{Layout Obfuscation}

This type of obfuscation involves modifying the layout of code, changing variable names and
generally making the program harder to read for a human. We have previously discussed programs that will
perform this type of obfuscation such as pyobfuscate.

The reason we are looking at layout obfuscation in our software is that we actually perform a very basic
form of this during the parsing and writing of our Python source. We will look at the transformation of the
program code to see why.

When writing a program there are many things a developer will include, often without even noticing, that
help a human reader to follow the program but do not affect the manner in which the code is run. Some of
these are obvious and designed for this purpose such as meaningful variable identifiers and program comments.

There are also more subtle features to the program source code. Whitespace is very helpful in many progamming
languages, this is especially true in Python as indentation is the method of identifying blocks of code, rather than
the familiar '\texttt{\{}' and '\texttt{\}}' or '\texttt{begin}' and '\texttt{end}' that we often see in other
languages. 

We do not just use whitespace to illustrate a program's nesting structure. This can be used for breaking long lines
into shorter readable ones, or for separating blocks of statements that perform separate tasks. For example, a block
of \texttt{import} statements in a source file will usually be followed by one of more blank lines to separate them
from the rest of the code.

While it is not strictly a layout issue, we can also see that the order a programmer writes code in is also important
to the human understanding of the program. There are often many different ways the software could have been written.
A machine would be indifferent to this, but one ordering may be much more intuitive that another. This will be discussed
in more detail when we come to reorder program statements.

Our contact with these matters begins when we parse a source file. By doing this we do not generate a representation of
the source code. Instead we create a representation of the program structure where the only information transferred to the
new representation is that which is necessary to run the program.

This means most importantly that we will lose comments. Immediately we have destroyed one of the most helpful tools
for understanding a complicated program. Of course, any comments contained in a docstring will still exist as these
affect the running of the program, however we will lose a great deal of information in spite of this.

All layout and whitespace will also be lost\footnote{Actually we retain column and line numbers for many nodes. We could
use these to recreate much of the original formatting, although the result would not be perfect.}, although we will retain the nesting structure and the tokens that the
whitespace was used to separate. As the results of the whitespace is retained, we can try to guess what combination
of characters generated our result, but we can never be sure.

So the only pieces of information retained from the source code are docstring comments, identifier names and the actual
program code, including structures and some meaningful statement order. These will be printed by our writers, along with
an estimation of how the layout used to look.

The effect of this process is that the the transformed code is actually being obfuscated. In fact we saw earlier the difference
in readability between our \texttt{BasicWriter} and \texttt{PrettyWriter} classes. Let us look at how much a source file
can be obfuscated just by passing it through \texttt{BasicWriter}.

We start with this contrived program:

\lstinputlisting[basicstyle=\small]{../snippets/layout.py.before}

After we pass this through \texttt{BasicWriter}, we end up with:

\lstinputlisting[basicstyle=\small]{../snippets/layout.py.after}

Without the information given in the comments, this code snippet no longer makes much sense. The change in layout also
makes the program harder to understand. We can see from previous examples and discussion that some of this obfuscation
can be mitigated by writing the program using \texttt{PrettyWriter}, therefore we also have a partial reverse obfuscator.

If we were to invest our efforts into actively creating a layout obfuscator, we could do much better just by implementing
a different writer class. For example a simple change would be to suites containing only simple statements all on a single
line, separating each by a semicolon. Alternatively we could instruct a writer not to write any docstrings, removing more
commenting, but possibly breaking the program.

This discussion will now be left as a possible extension of the software, and we can concentrate on our main goal;
control-flow obfuscation.

\subsection{Control-flow Obfuscation}

As we have discussed already, there are a number of types of control-flow obfuscation available to implement. We should
remember now that we are not solely looking to obfuscate source code, but also to deobfuscate it ourselves as part of
our evaluation.

If we look through the previous list of example control-flow obfuscations, we can see there are a mix between those
obfuscations that are easy to perform but involve serious work to remove, those that perform complicated work to perform
but can be removed simply, and those that involve similar work in either direction.

If we look at any transformations relying on opaque predicates, we see that these tend to be simple to perform, but the
reversal requires careful analysis to locate and identify opaque predicates. Obviously the difficulty of reversal depends
on the quality of the predicate, so the main difficulty in the forward step will be to construct these well. These types
of obfuscations include adding dead code, extending loop conditions, or adding redundant operations to expressions.

Pattern recognition within the code will be one technique we will rely on for some obfuscations. This will apply both for
outlining and removing or replacing idioms. Of course if a pattern is broken, such as when removing idioms, the result can
be further obfuscated, hugely complicating the task of recognising the original pattern.

We could extend the notion of removing idioms and deconstruct many useful constructs such as list comprehensions or generators.
The semantics of such are well documented \cite{genexpr}, so we should find it easy to create an equivalent program without
these constructs. Again we are already removing some constructs such as \texttt{elif} during parsing as we described earlier.

Table translation would be simple to implement in CPython as the standard library actually includes a module called \texttt{dis}
to interpret (and compile programs to bytecode). Unfortunately as \texttt{dis} is so tied into CPython, it is not available in
other runtimes such as Jython, therefore if we use this method, we will need to invent our own virtual machine language and
produce bytecode and an interpreter for it. We will not be using this method. 

If we were to parallelise our code, this could be done using the \texttt{multiprocessing} standard library. This makes it
very simple to spawn a new process using \texttt{Process} and exchange objects with \texttt{Queue} or \texttt{Array}. By
performing redundant operations on these objects in both processes, we make the analysis required to re-merge the processes
very difficult.

Finally, one ability required explicitly for many types of control-flow obfuscation, as well as implicitly for many of those
described above is the ability to resolve variable values. This is needed for tasks such as removing or replacing library calls
as an imported standard library could be assigned to any variable at all, or standard library names could be used to hold other
data. This means we need to resolve any identifiers to discover whether a call is actually directed to a library or not.

A subset of this ability is also needed when moving and rearranging parts of the program. We do not need to know which value a
particular identifier refers to, but we do need to know which identifiers are used in which parts of a program and which
identifiers refer to the same data.

The same set of tools are necessary both when obfuscating and deobfuscating a program by rearranging its components. Knowing
this, we can implement the transformation in one direction and already have the tools to be able to implement the reverse.
This is why we will implement program reordering as our first obfuscation, specifically reordering statements rather than
files, as our parser currently only handles a single file at a time.

Before we skip straight to implementing the transformations, we must make sure that we can perform the necessary analysis. As
most information we calculate will be useful to many different forms of obfuscation, we should make sure it is only calculated
once and can be stored and recalled whenever needed.

This can be done by 'marking' the AST with our calculated results.

\section{Marking the Abstract Syntax Tree}

Before concentrating on the design and implementation of analytic tools for our program, we shall provide a method of storing
the results. By attaching data straight onto individual nodes in the AST, or 'marking' them, we make it very easy to extract
relevant information as we process that particular node.

It is worth noting at this point that the design of the marker code was created before the \texttt{CustomAST} class was. This
means that we are working on ASTs where nodes may be either \texttt{ast.AST} nodes, lists or basic types such as strings or
integers.

\subsection{Implementing Marking}

When thinking of how to attach the data to these nodes, we cannot alter the underlying class or type as these are generated directly
from the \texttt{ast} module's \texttt{parse} method. Instead we expect to add new attributes to individual objects that we can read
and write to.

The difficulty in this assumption is that not all nodes support adding these. In fact only the \texttt{ast.AST} nodes will allow this
addition and while they do make up the majority of most ASTs, this will cause difficulty for many algorithms trying to process the
nodes in a standard way.

To deal with these differences and with all of the work involved in deciding if a node has been marked and by which type of markings,
we will create a new class; \texttt{BasicMarker}. To clarify, a type of marking is defined by the information it stores, for example
we will create a 'reads' marking that contains a set of identifiers a node reads.


\texttt{BasicMarker} is designed as disposable class through which a specific node can be temporarily viewed. It is designed as a base
class with functions common to any type of marking, allowing other classes to inherit the functionality and add methods specific to a
type of marking. While subclasses for each type of marking will be separate, multiple types of marking should be able to coexist on the
same node. \texttt{BasicMarker} exposes the following functionality:

\begin{description}
\item[\texttt{\_\_init\_\_(mark, node)}] \hfill \\
This constructor is designed for a subclass to call, supplying the name of their specific type of marking and a node to view. The only
processing done here is to test if the given node is \texttt{None}, and if so to replace it with a \texttt{DummyAST} node.

\texttt{DummyAST} is merely an identical subclass of \texttt{ast.AST}. This allows markings to be created without a node and applied later,
without causing errors when a node derived from \texttt{ast.AST} is expected.

\item[\texttt{supports\_markings()}] \hfill \\
This checks whether the given node supports markings. As we have seen, this is true if and only if we are looking at an \texttt{ast.AST}
node.

\item[\texttt{has\_markings()}] \hfill \\
All types of markings will be stored together in an dictionary attribute called \texttt{\_markings}. This should map the name of the marking
type to an arbitrary object that completely describes the data attached. We can see if the node has ever been marked by testing for the
existence of this attribute.

\item[\texttt{\_get\_markings(create)}] \hfill \\
This is a private method, designed only for use inside this class. For convenience, this will retrieve the \texttt{\_markings} attribute
from the node if it exists, otherwise it will return \texttt{None}. If \texttt{create} is set to \texttt{True} and the node supports
markings, then we we can first add this attribute and then return it.

\item[\texttt{is\_marked()}, \texttt{get\_mark()} and \texttt{set\_mark(val)}] \hfill \\
These are designed to be called from derived classes. The first two will try to find the name of this particular type of marking inside
the \texttt{\_markings} dictionary. \texttt{is\_marked} will tell us if this was successful, and \texttt{get\_mark} will return either the
corresponding value, or a safe default on failure. \texttt{set\_mark(val)} will try instead to set the value and return a boolean value
indicating if it was successfull

Only \texttt{is\_marked} is designed to be public. \texttt{get\_mark} should be used for convenience in subclasses that will provide
customised interfaces to help interpret the particular type of marking.

\item[\texttt{detach()}] \hfill \\
If we begin viewing a specific node we can use this method to forget about that node, transplanting the current markings onto a new
\texttt{DummyAST} node.

Again this method should be used only from derived classes. It will not move the entire set of markings, just the type defined
in the constructor. We rely on the \texttt{duplicate} method to properly clone the markings.

\item[\texttt{get\_default()}] \hfill \\
We can use this method to find a safe default for the given type of marking. It is actually an abstract method that needs implementing by
subclasses.

\item[\texttt{duplicate()}] \hfill \\
Another abstract method. When implemented, this should clone the data returned by \texttt{get\_mark} in a way that allows updating the
data without altering our original node.

\end{description}

After the introduction of the \texttt{CustomAST} class as our AST storage vessel we can modify this design subtly. Really this just
involves replacing \texttt{DummyAST} with \texttt{CustomAST(None)} and allowing \texttt{supports\_markings} to always return
\texttt{True} as all nodes should be \texttt{CustomAST} nodes.

There would be more sensible ways to redesign the marking code now that we are using \texttt{CustomAST}. Unfortunately we do not have the
time to do this, so we will continue to use the slightly ill fitting original design.

It should be clear that this class is useless on its own. Solid subclasses will be defined and created as we need them; during the
development of the analysis that requires them. We can now begin to design and implement the tools for this analysis.

\subsection{Automatic Marking}

During this period of design, we will assume that we already know of two types of markings that wish to mark nodes with. These are visible nodes
and flow-breaking nodes. Saying that a node is visible loosely means that it can produce output that a user will see, for instance an \texttt{ast.Call}
node that calls the \texttt{print} method. Being a flow-breaking node means that the node can cause a break from the linear flow of the program. These
will be described in more detail in the next section.

While both the visibility and flow-breaking markings can be queried for a boolean 'yes' or 'no' value, the total data stored relating to breaks is much
larger. Rather than just recording whether a node can break linear flow, we also record how; either by an escaped \textbf{except}ion,
\textbf{return}ing or \textbf{yield}ing a value, or performing a \textbf{break} or \textbf{continue} from a loop. This eases our task of automatically
marking nodes, as parent nodes will react differently depending on the type of break.

Now we can use these two marking types to guide our creation of an automatic marking tool. First we should think about which options are available
to us when choosing the value for these markings. A simple start would be a safe default, this would the the same for any node in any position,
depending only on the type of marking. Also simple would be asking a user to decide the value, allowing them to inspect details of the node before
making the decision.

Of course if the node could possibly have already been marked, we can just take these current markings. The final option we will think about is
that we could calculate the values based on those of any child nodes. Any of these four options may be combined into a hybrid automarker, and actually
this is what we will do.

The idea is that to calculate the value for marks on a single node, we should first order the discussed methods of choice into a mark resolution
list. We can then proceed through these one by one until we are provided with an answer. They will be labelled in the software as
\textbf{default}, \textbf{user}, \textbf{mark} and \textbf{calc}. It is worth noting that \textbf{calc} is the only recursive resolution method,
and that it will check the markings on child nodes by proceeding through our entire resolution list again.

To see how this will work, we will set up an example with a resolution order of \textbf{mark}, \textbf{user}, \textbf{calc}, \textbf{default}.
We want to add visibility and flow-breaking markings to a \texttt{list} node representing the following sequence of statements:

\lstinputlisting{../snippets/automark.py}

We can now run the algorithm to discover the appropriate set of markings to use.

First we look at the resolution order. This begins with \textbf{mark} so we check for existing markings. As we haven't marked this node before,
we have none and so must check the next method in the list. This tells us to ask the user who informs us that they do not know or they would not
have used an automatic marker.

Now the next resolution method is \textbf{calc}. We will need to know the value of the markings on each of the children to work this out. These
are the assignment and the \texttt{if}-\texttt{else} statement, so we must perform the same calculation for both of these.

\begin{description}
  \item[\texttt{ast.Assign}: \texttt{wound = get\_wound()}] \hfill \\
  Again we check markings and see that there are none. This will be the case for all nodes here and so we will not write this step again.
  Next we ask the user, who does not want to think about this so again we turn to \textbf{calc}.
  \begin{description}
    \item[\texttt{ast.Name}: \texttt{wound}] \hfill \\
    Actually the real child would be a list of targets for the assignment. As there is only one we have skipped a step for simplicity.
    
    We ask the user for markings who assumes they know that a basic name will not cause visible output or break flow. Out of interest they still
    decide to pass and see what \textbf{calc} has to say.
    \begin{description}
      \item[\texttt{str}: \texttt{wound}] \hfill \\
      The user decides to follow their previous instinct and decides that a simple string cannot really produce anything by itself. This node is
      marked as non-visible and non-breaking.

      \item[\texttt{ast.Store}] \hfill \\
      The user has no idea what this node is so passes. Now our knowledgeable automarker knows that this type of node is used to represent the context
      a name is used in. This particular node is describing a name being used to store data and so could cause an exception if the name is not writable.
      
      Now we can mark this node as non-visible but possibly breaking with an exception.
    \end{description}

    As neither child is visible we can probably assume that our \texttt{ast.Name} node is also non-visible. However as one of the children could cause an
    exception that we will definitely not catch (as this node is not a \texttt{try}-\texttt{catch} node), we will mark this node with the possibility
    of causing an exception.

    \item[\texttt{ast.Call}: \texttt{get\_wound()}] \hfill \\
    Now our user does not know about the \texttt{get\_wound} function so passes the marking to our automarker. Our automarker knows it does not have
    the ability to resolve the name being called and so also passes.

    This means we will mark this node with safe default values which are set in the corresponding marker classes. Hence our node will be marked
    as visible and possibly flow-breaking by exception.
  \end{description}
  This assignment contains both a visible child and a flow-breaking child. If we let these markings bubble through, then we can mark our assignment
  as both visible and flow-breaking by exception. 

  \item[\texttt{ast.If}] \hfill \\
  Now we could calculate the markings for this \texttt{if} statement here. Unless it turns out to be flow-breaking due to something other than an
  exception, we will find this task to be pointless and unable to affect our parents markings. It may be possible to use this type of reasoning
  during the implementation to avoid excess work.

  In this case however, we will allow the user to mark the node simply as non-breaking but visible due to the \texttt{print} statement.
\end{description}

We have now returned to our original node. We know that we have a visible child and a flow-breaking child, so again we let these bubble through
and have a final marking of visible and flow-breaking by exception.

Some readers may have noticed a few problems with this example, mainly that a sequence of statements that we would not expect to cause an exception
have been marked to indicate that they could. The reason for this is that actually there are a number of places this small program \textit{could} cause
exceptions, most notably at any point where we need to resolve a name, or when unknown functions are called.

If fact almost any line in most Python programs could raise exceptions and most will not be marked in any way within the code. This is not usually a
problem for humans, as a programmer tends to program in a way that avoids implicitly raised exceptions such as those raised by using an undefined name,
and explicitly raise exceptions are easy to spot. Humans will also tend to ignore system exceptions when programming, such as those raised when we run
out of memory as these indicate that something is actually broken and tend to stop the program immediately.

Unfortunately our automarker does not benefit from this human intuition. The last thing we wish to do is to break a program by assuming something that
could happen does not. This means that if a node could break flow or produce visible output we must mark them as so, even if it would be unlikely to ever
happen. Of course if we can somehow prove that an exception will not be caused, such as by showing that a name is assigned to before it is read from,
then we can mark the node as such.

Most forms of analysis required for this type of proof will be difficult to perform, so for now at least we will be reliant on our users. During the
algorithm described above, we are currently asking the user only once to provide a set of markings for the node. We will now introduce a review step
after markings are generated for a node, but before they are written to it. This allows a user to inspect the generated markings and reject them if
they feel them to be incorrect.

As well as the user review step, we will also allow the user to edit markings completely manually. This gives us the benefit of both the expert
knowledge and speed of the automarker, but also of the intuition of the user. As the automarker improves, reliance on the user will diminish, allowing
gradual evolution of the tool.

Now we must implement our algorithm, clarifying our ideas in the software.

\subsection{Implementing the Automarker}

The main class to be implemented is the \texttt{AutoMarker} class, this implements all of the functionality needed to create a hybrid automarker.
We define the class as so:

\begin{description}
\item[\texttt{\_\_init\_\_(res\_order, mark, user, review, defaults)}] \hfill \\
The constructor takes a large number of arguments, but nevertheless is fairly simple.

\texttt{res\_order} defines the resolution order we will use. Rather than ordering all four of \textbf{mark}, \textbf{user}, \textbf{calc} and
\textbf{default}, we cleverly notice that \textbf{default} will always return to us a value for the markings. This means that any resolution methods
occuring after \textbf{default} will never be used and can be forgotten. We also decide to make sure that \textbf{default} is always implicitly in
our list, allowing our recursive marking method to always return a value. Thus we order only \textbf{mark}, \textbf{user} and \textbf{calc},
represented by a list of strings.

We also take the parameter \texttt{mark}, indicating what to do when have calculated a set of markings for a node. Any markings should be gathered using
the \texttt{resolve\_marks} method rather than reading from the node, so we do not necessarily need to write the markings once they are calculated. By
setting this parameter to \texttt{False}, we can calculate markings without affecting the tree in any way.

\texttt{user} and \texttt{review} are two functions that will be used to ask the user for markings, and to review those markings, respectively. These
cannot be included in the class as \texttt{AutoMarker} is designed to be interface-independent so interaction with a user needs to be dealt with by
the calling code.

Finally \texttt{defaults} is a dictionary mapping marking types to functions that generate default values. We cannot simply provide the default values,
as they may be mutable. This would mean that if node's markings are ever changed after being given this default, all other markings with these default
markings would be changed too.

\item[\texttt{resolve\_marks(node, needed)}] \hfill \\
This is the method that should be used both internally and externally to resolve markings on a specific node. We supply \texttt{needed}, a set of strings
indicating all of the types of marking we wish to resolve. The returned value is a dictionary that is guaranteed to contain markings for each of the
needed types.

The internal workings of this method are simple. First we create an empty dictionary of markings. For each resolution method in our specified resolution
order, we look at all the types of markings we already have and remove them from \texttt{needed}. Now we ask the current resolution method for all the
markings in the \texttt{needed} set that it can give us, add these to our marking dictionary and repeat.

Once we have been through the resolution list, recall we implicitly finish with \textbf{default} so now remaining needed markings are filled by defaults.
To finish we call the user review function and if marking was turned on in the constructor, we will write the calculated markings to the node. We can now
return our results.

\item[\texttt{\_base\_marks(needed)}] \hfill \\
This should return the set of markings that we would use to mark an empty program. These should be able to be used as an identity element for
\texttt{\_combine\_marks}, meaning that their combination should not affect the original markings. Obviously only those marks whose types are in
\texttt{needed} will be returned, we will assume this is true for all functions taking this parameter.

\item[\texttt{\_combine\_marks(marks, addition, needed)}] \hfill \\
Often we will have to combine the markings from many nodes into a single set of markings. This method assumes we have already combined a number of nodes
which result in the markings represented by \texttt{marks}. Now we expect that the next node to be interpreted or executed would generate the markings
in \texttt{addition} when executed by itself. This method will combine both of these sets of markings and return the result.

\item[\texttt{default\_marks(node, needed)}] \hfill \\
This implements the \textbf{default} resolution method. It will call the default methods we gave in the contructor to generate a dictionary containing
markings for all \texttt{needed} types.

\item[\texttt{get\_marks(node, needed)}] \hfill \\
This is a very simple function, implementing the \textbf{mark} resolution method. All we do is retrieve the \texttt{needed} types of markings from the
given \texttt{node} if they exist and return a dictionary containing them. This method is not required to return a marking for every \texttt{needed}
marking type, just like \texttt{calculate\_marks} below.

\item[\texttt{calculate\_marks(node, needed)}] \hfill \\
Here we implement the resolution method, \texttt{calc}. This is an indirectly recursive method but is guaranteed to terminate so long as the node
given contains no reference cycles. This should be true as the node should represent a tree.

We can quickly show this property as \texttt{calculate\_marks} calls itself through \texttt{resolve\_marks}, but only on child nodes. Therefore we have
a base case where our node has no children and so \texttt{calculate\_mark} must terminate. As the AST does not loop and it must be of finite size,
eventually at some level of recursion we will reach a terminal node, whichever path we take down the tree. Working back up the call tree we can see
that any call to \texttt{calculate\_marks} must terminate.

There are multiple phases of design for this function which we can now describe.
\end{description}

\subsubsection{Implementing Marking Calculation - First Attempt}

We already know the structure we expect from our mark calculation. The process starts by calling \texttt{calculate\_marks}, and this function will
need to try and produce markings for each type in \texttt{needed}. To do this we expect to use the markings from various children, depending on the
type of node we are looking at, to work out the markings for the whole node.

As our method of marking differs based on the type of node, it makes sense to think back to where we have used this pattern previously. We will begin
implementing this function in the style we used to produce our \texttt{SourceWriter} and derived classes. Therefore \texttt{calculate\_marks} becomes
a dispatcher function, passing any work onto a method named \texttt{\_marks\_\{node type\}}.

One of these methods is implemented for every type in our AST. If we have forgotten or decided not to implement one of these then rather than raising
and exception as we would in \texttt{SourceWriter}, we just return no markings. This allows us to just ignore node types we are unsure of and allow
the other methods in the resolution list to fill the gap.

While this design works correctly, and is flexible enough to allow almost any processing we would like to do, we soon notice flaws for the developer.
Flexibility is a useful attribute to have, but it is wasted once we notice that most of these nodes are processed in very similar manners, only using
different child names.

We also have the problem that having this many tiny functions creates a very messy codebase. Most of these may be written in one or two lines, and those
that take more can usually have their contents outlined into a common function. These problems give rise to our second design.

\subsubsection{Implementing Marking Calculations - Second Attempt}

This time we will try to keep hold of the good parts of the last design, correctness and flexibility, but fix the problems of code size and functionality
duplication. We would like to be able to fit much more code into a single screen without it looking messy. By doing this, a developer would find it much
easier to read through the program and immediately see how the algorithm will behave for each node type.

To deal with code duplication we will try to categorise all of the different tasks our first version performed. These are:

\begin{itemize}
\item Using identical markings to a child node. This happens when processing nodes like \texttt{ast.Expr} where its single child defines it exactly.
\item Combining markings from child nodes. We use this in places such as lists of statements.
\item As above but adding or removing flow-breaking exceptions. For instance when processing an \texttt{ast.For} node whose body contains a \texttt{break}
      statement. This is flow breaking inside the loop but will not continue past this.
\item Transforming the node into a new node and using the markings gained by analysing the new node. We can use this to simplify processing of functions
      or classes with decorators.
\item Some combination of the above, but ignoring certain types of marking that we cannot calculate . This helps more with some of the marking types we have
      not added yet.
\item Take a set of markings from children as above but process them to assume that it is only a possibility this will happen. We see this in \texttt{if} or
      \texttt{for} statements whose body may never be run.
\end{itemize}

None of these tasks are related to visibility markings. This is because we cannot tell if a node is visible just from its type or content. We rely on the
other resolution methods to mark child nodes as visible, which tends to follow through to the parent.

Now we can think about how to pull all of our small functions together and merge their work to create a clear and concise process for each node type. Our idea
is to create a single function that can perform any of these tasks. Rather than putting work into individual functions we build a data structure to describe
the work we wish to do, call this a task description, and let our single function interpret and carry this out.

Let us first define the task description, and then we can describe the implementation or the interpreter. We will have a different task description for each
type of node, these are held in a dictionary. A single task description will be allowed to be one of four types:

\begin{description}
\item[Dictionary] \hfill \\
This will describe the main part of any calculations. It maps a number of tasks onto the parameters they require. To interpret this we will
start with a set of marks generated by \texttt{\_base\_marks} and perform all defined tasks on these. The available tasks are:

\begin{description}
\item["local" : set containing names of relevant children] \hfill \\
Used to combine the markings from each of the given children, assuming that they are all in current scope.
\item["localg" : set containing names of relevant children] \hfill \\
Exactly as above but each of the children are expected to be lists from which we can treat each item separately.
\item["add\_break" : set of types of breaks] \hfill \\
Add each of the given break types to the set of possible breaks.
\item["rem\_break" : set of types of breaks] \hfill \\
Remove each of the given break types from the set of possible breaks if they are there.
\item["transform" : function] \hfill \\
If this exists, it will always be performed first. The current node will be passed to the function and our next action depends on the returned value. On a
\texttt{None} return value, we will continue as normal. Otherwise we will assume the value is a new node and we treat the markings generated for this as the
\textit{entire} set of markings.
\item["unknown" : set of marking types] \hfill \\
We assume that we do not know the given set of marking types, so they are not returned.
\item["known" : set of marking types] \hfill \\
As above but we define only those marking types we do know.
\end{description}

\item[Callable] \hfill \\
This will be called and given our current node. It should return a dictionary as above, allowing us to change the task depending on specific parts of the node.

\item[List of Dictionaries] \hfill \\
Each dictionary should be as defined above and will all generate a separate set of markings. These will then be combined into a single set of markings allowing
us to perform different tasks to generate different types of markings when combined with the \textbf{unknown} and \textbf{known} options.

\item[Single Item Tuple] \hfill \\
This should contain a single dictionary and will first generate the markings according to that dictionary. These markings will edited to treat them as if
it was only a possibility that they would happen. The tuple was chosen as it is represented by bracketing the original dictionary.
\end{description}

From this definition of a task description, we can easily see how the interpreter would be implemented. Now both implementation of the general tasks, and
the definition of specific task descriptions become much simpler. We can even read through the task descriptions and understand quickly what is happening
as required. For example the following is the task description for a return node.

\lstinputlisting{../snippets/task.py.return}

We quickly read from this, that in order to generate markings for a return node we use the markings from the child "value", and add a "return" break.

Again we should be critical of this design. The objectives here were to design a flexible system with a small codebase, and these have been achieved.
However we also wanted to retain correctness and flexibility, of which we have lost the second. We also notice that although we have simplified the
original design for a user reading the code, the actual design is not simple at all. So now we try again.

\# We designed the structure based on the current functions, fix it for later ones!

\# We can now create a command for oat to extend this functionality to the user. This will mean that a user can experiment and alter markings
\# without prior programmatic analysis.

\# \subsection{Extending Marking Functionality to the User}

\# Develop mark command

\section{Reordering Program Statements}

We are now able to think about implementing our first type of obfuscation; reordering program statements. This will involve two steps,
firstly we need to decide on a definition for a legal reordering and be able to generate these. Following this we can decide on how to
choose the 'best' order to either obfuscate of deobfuscate the program.

\subsection{Why Reorder Statements?}

As discussed in previous sections, a well-written program will usually contain many clues to indicate the programmer's thought process
at the time. This is not a problem, in fact it helps other developers to follow the code and contribute with their own input. 

Unfortunately, when the software is distributed in source form, this will help a reverse engineer to do the very same thing. As we do not want
this to happen, we will try to remove as many of these clues as possible. We shall start by looking at the organisation of the program source.
This falls broadly into two categories; the layout of of the source code in each file, and the location and order of its components. 

We have already seen that most of the layout is destroyed during parsing, any whitespace will only be recoverable as an estimate of the original.
Therefore we may focus on how to change the order and location of program components, restricted by the design of our tool to performing these
changes within a single source file.

The reason we are reordering statements becomes apparent when we start to think about what each program component is. We may first think about
program source at module level, the top level of a source file. This will usually contain a combination of \texttt{import} statements from
other files, class definitions, function definitions, and normal program code.

In Python, a function or class definition is considered to be a compound statement, just like an \texttt{if} or \texttt{for} statement is. This
means that the entire module level is made up of a series of statements. In fact this is exactly what the Python language reference tells us, that
any line may be either blank or a statement \cite{fileformat}.

Now we can reorder a program at module level just by reordering individual statements. It would be possible to implement this and be satisfied with
ourselves, but we can do much better. Each statement can be classified as a simple statement such as an assignment or \texttt{import} statement,
which are of no further use to us, or as a compound statement.

Compound statements on the other hand are very useful to us. These include \texttt{if} or \texttt{while} statements and each includes a suite, or
a list of statements, exactly as the module did. Now using the tools we will implement to reorder the module level of a program, we can also reorder
the body of any compound statement. This will allow us to rearrange most of the program by thinking only of a much simpler case.

Up until this point we have been thinking about reordering statements as a way to take information away from a reverse engineer. The idea was that
the original source code would be organised in a logical order, and that related components of the program would likely be close together, in an intuitive
order for a human. By reordering the program we are destroying that information, and also making it difficult to recreate the original source structure. For
example it would be difficult to separate logical sections of code with whitespace when each section is interleaved with the others. While it is true that we
are mostly mystifying a human, we should also see benefits of our obfuscation against a deobfuscator.

Notice that there is no single correct order to write a program in. As long as the ordering produces the correct results, it is valid. One may argue that
some orderings are better than others, for instance one ordering may be easier to understand or read, or may run faster than another, but there is not
a single correct answer. This means that a deobfuscator cannot find a single correct ordering, and even if it looks for the best, this may not result in
the program we started with. We can then say that this type of obfuscation is irreversible in general, a very nice propery for any obfuscation. 

Now being irreversible is very good, but actually we would not care if it did not also bring some benefit in terms of obfuscating the code. Actually
reordering statements comes in very handy when combined with other types of obfuscation. We will illustrate this with an example, from the background
research, of an obfuscation technique involving replacing common convenient constructs with more complex representations of themselves.

If this obfuscation was performed by itself, an analyser should find it very easy to recognise the common pattern of the deconstructions of each of these
constructs. This gives the obfuscation by itself a very low resilience, as there is little effort invested in removing it. Instead may we choose to replace
these constructs and then reorder all of the statements in that block. Suddenly it is much more difficult to recognise the common pattern, and as we cannot
accurately reverse the reordering, it is difficult to reverse the steps to find the original program.

\subsection{Specifying the Problem}

So far we have been rather vague about the task we wish to perform. If we are to implement tools for reordering program statements we must now
define exactly what we mean to do.

During the previous section we discussed the meaning of a program statement, this can be anything from an assignment to a function definition.
In terms of our AST we can say that any node derived from the \texttt{ast.stmt} class is one which represents some form of statement. From Python's
abstract grammar \cite{pyagrammar} we can see these are:

\begin{description}
\item[\texttt{ast.FunctionDef}] \hfill \\
Definition of a function. Possibly multiline: \texttt{def f(params): pass}
\item[\texttt{ast.ClassDef}] \hfill \\
Definition of a class. Possibly multiline: \texttt{class C(bases): pass}
\item[\texttt{ast.Return}] \hfill \\
Simple return statement: \texttt{return x}
\item[\texttt{ast.Delete}] \hfill \\
Removal of a name binding: \texttt{del x}
\item[\texttt{ast.Assign} and \texttt{ast.AugAssign}] \hfill \\
Normal or augmented assignment: \texttt{x = 1} or \texttt{x += 1}
\item[\texttt{ast.For} and \texttt{ast.While}] \hfill \\
For or while loop. Possibly multiline: \texttt{for x in range(y): pass} or \texttt{while x: pass}
\item[\texttt{ast.If}] \hfill \\
If statement. Possibly multiline: \texttt{if x: pass}
\item[\texttt{ast.With}] \hfill \\
With statement, used for convent set up/tear down such as when opening files. Possibly multiline: \texttt{with open(x) as file: pass}
\item[\texttt{ast.Raise}] \hfill \\
Raise statement, raises exception: \texttt{raise x}
\item[\texttt{ast.TryExcept} and \texttt{ast.TryFinally}] \hfill \\
Exception catching. Possibly multiline: \\
\texttt{try: pass} \\
\texttt{except: pass} \\
\texttt{finally: pass}
\item[\texttt{ast.Import} and \texttt{ast.ImportFrom}] \hfill \\
Import other files: \texttt{import tkinter} or \texttt{from tkinter import ttk}
\item[\texttt{ast.Global} and \texttt{ast.NonLocal}] \hfill \\
Assign a specific scope to an identifier: \texttt{global x} or \texttt{nonlocal x}
\item[\texttt{ast.Expr}] \hfill \\
Simple expression: \texttt{x}
\item[\texttt{ast.Pass}] \hfill \\
Do nothing: \texttt{pass}
\end{description}

Now we will define our reordering operation to be on any list of these statements, so long as the list represents the serial execution of each in turn.
Such lists are found representing suites in any compound statement, or the body of a module for instance.

\subsection{Defining Legal Orderings}

We know now that we are focusing solely on reordering a linear sequence of statements. At this point we must try to form a definition of a legal ordering
for the sequence. From this definition, we may draw an implementation of a generator for such orderings.

Let us think about what happens when we run the following sequence of $n$ statements.

$$S_1 \; S_2 \; S_3 \; \ldots \; S_{n-2} \; S_{n-1} \; S_n$$

Beginning at some specified state inside and outside of the program, usually each $S_i$ is executed in turn until we reach $S_n$. There are certain cases where
this does not happen, the first being in the event we encounter a flow-breaking statement. These are statements that will jump from the linear flow, for
example a \texttt{return} statement or any statement that can cause an exception. The other case occurs when a statement does not ever terminate.

For now we will only consider sequences such that for any initial state, the sequence will always terminate, and statements will never be flow-breaking. This also
means that each of the individual statements must terminate. We prove this by assuming one or more of them do not. As we execute each statement in a linear
order, and there are no flow breaking statements, we must eventually execute the first of these non-terminating statements, hanging the program indefinitely.
As the sequence terminates this is a contradiction, meaning that every statement must terminate.

We consider the following sequence of statements to be of this type.

$$T_1 \; T_2 \; T_3 \; \ldots \; T_{m-2} \; T_{m-1} \; T_m$$

Again we consider the execution of this sequence, starting with a specified world state and executing each statement in turn. As these statements all terminate,
and there are no flow-breaking statements, we can see that every single statement is executed in order. During this execution, we will may notice changes to state
outside of the program, as well as the time between these changes. At the end of the execution we can see the total time of execution and the rest of the program
will see the internal state left by the executions.

As execution time is very much dependent on the execution environment and difficult to measure without running the program, we have decided to ignore this aspect
of our statement sequences. Thus during the execution of any legal ordering of the sequence, we wish to preserve changes to the outside world, and their order,
as well as the internal state left after the execution.

The first part of our definition of a legal reordering of $T_i$ now says:

\begin{description}
\item[Rule 1] \hfill \\
For any reordering, all $T_i$ that make changes to the world outside of the program must remain in the same order relative to each other.
\end{description}

Given this part of the definition, we just need to ensure two more things for our legal ordering; for any reordering we must preserve the final internal
state as well as the effects of every $T_i$ on the outside world.

Both of these constraints can be tackled using the same method. This will be to ensure that before any statement in the sequence is executed, any internal
or external state that is read by the statement is identical to that which we would have seen in the original ordering. Then we can be sure that any statement
will produce exactly the same effect before and after the reordering. We can ensure that the final state is preserved by treating the end of the sequence as
a stationary statement that reads everything. We do this without any knowledge of the outer world or of the inner workings of the statement.

Let us now add two empty pseudo-statements $T_{start}$ and $T_{end}$ either side of our current statement sequence. $T_{start}$ is assumed to write to every
region and $T_{end}$ is assumed to read every region. These statements will not be reordered with the rest, and we can now write our sequence as:

$$T_{start} \; T_1 \; T_2 \; T_3 \; \ldots \; T_{m-2} \; T_{m-1} \; T_m \; T_{end}$$

The only information that have about these statements tells us which parts of the internal state each $T_i$ reads or writes during its execution. We will
also assume that we know if a statement reads or writes to the outside world, but do not care which part of this is interacted with. To record this we
will split the internal state into separate regions, and assume that every write will replace the entire region with what it is writing. If this is not
the case then we can think of the statement as first reading the region and then using this data to write to its entirety.

We look at the external state first, noticing that the statements that change it are all those $T_i$s mentioned in \textbf{rule 1}. If any statements
read from the external state, each of these will need to remain in order relative to the $T_i$s above. We will place a slightly stronger constraint on these
and instead write:

\begin{description}
\item[Rule 2] \hfill \\
For any reordering, all $T_i$ that make changes \textit{or read changes} to the world outside of the program must remain in the same order relative to each other.

Call these statements \textbf{visible} statements.
\end{description}

Now we see that as a write to any region rewrites the entire region, our initial state condition can be simply fulfilled. First we will define for some region
$r$ and a statement $T_i$, that the \textbf{providing} statement of $r$ to $T_i$ is the closest statement to be executed before $T_i$ that writes to $r$. Now
we can write:

\begin{description}
\item[Rule 3] \hfill \\
Fix $i \ne {start}$ then for any region $r$ read by $T_i$, the providing statement of $r$ to $T_i$ should be the same before and after reordering.
\end{description}

These 3 rules alone will now give us a valid ordering, ensuring that changes to the outside world will be performed in order and exactly as they were before.
This will also ensure that the internal state left after the sequence has executed will be the same before and after the reordering. This assumes we are working
on our $T_i$s including $T_{start}$ and $T_{end}$, so all statements must terminate and cannot be flow-breaking.

Now we must extend our set of rules to cover sequences of statements that are possibly non-terminating or flow-breaking. It is possible to introduce non-termination
simply by treating non-terminating statements as visible statements. If a non-terminating statement is encountered, we can guarantee that all visible statements that
would have been executed by this point in the original ordering still will have been in a legal reordering.

Treating statements that execute slowly as being visible too also solves a problem we would have gained by ignoring execution time earlier. A common
pattern in software would be to warn user that a slow operation is about to occur, perform the operation, and then possibly let the user know the
operation is finished. We would usually expect that the user warnings would be visible statements, but they would not rely in any way on the slow
operation and so would have no reason to stay either side of it. By treating the slow operation as visible, we create this relationship and preserve
the order. This does make sense as a slow operation will usually become very apparent or visible to the user.

The way we will introduce flow-breaking statements is slightly more complicated, though not by much. If we assume that a flow-breaking statement
\textit{always} breaks the linear flow, we may decide legal orderings for the statements following could be anything. This works for most types of
breaks, although a \texttt{yield} statement will break flow while allowing execution to resume from the point at which we left. This means if our
breaking statement is breaking due to a \texttt{yield}, we have to make sure the ordering following this statement is legal as defined earlier.
For convenience we will treat all breaking statements the same as our \texttt{yield} statement as it is also how we will treat statements for which
breaking flow is only a possibility.

We know how we will treat statements following a flow-breaking statements, now we think about those before. We know we have to make sure visible
statements before or after the break stay before or after the break. Also we know that program state should be left the same when the program breaks
in whichever legal order we have placed the statements. To achieve this we will use the following final rule:

\begin{description}
\item[Rule 4] \hfill \\
Rules 1 to 3 apply only to non-breaking statements. For a sequence containing flow-breaking statements, partition the sequence so that each partition
contains either a continuous run of non-breaking statements or a single breaking statement. A legal ordering is now some combination of legal orderings
of each individual partition.
\end{description}

To illustrate this imagine the following sequence of statements:

$$S_1 \; S_2 \; B_3 \; S_4 \; \ldots \; S_{n-4} \; B_{n-3} \; S_{n-2} \; S_{n-1} \; S_n$$

where $S_i$ and $B_i$ represent non-breaking and breaking statements. These would be partitioned into:

$$S_1 \; S_2 \text{ and } B_3 \text{ and } S_4 \; \ldots \; S_{n-4} \text{ and } B_{n-3} \text{ and } S_{n-2} \; S_{n-1} \; S_n$$

Legal orderings will now be

$$\text{reorder}(S_1 \; S_2) \; B_3 \; \text{reorder}(S_4 \; \ldots \; S_{n-4}) \; B_{n-3} \; \text{reorder}(S_{n-2} \; S_{n-1} \; S_n)$$

We can see that this rule works because the execution of any legal reordering of a partition will result in the same final inner and outer state.
This happens to be exactly the state required by the following partition. Also we can see visible statements will remain in the same order as
partitions are not moved relative to each other.

While we have shown that these rules produce correct orderings for any statement list, with respect to the definition of correct we gave earlier,
we will not always produce a complete set of orderings. This is due to a number of small concessions we made during the reasoning process. While
this is not perfect, it is a reasonable compromise as to gather a complete set of reorderings for any sequence could require very thorough analysis,
calculating variable values at various points in the program. This is simply not practical for our task so we will settle with our incomplete but
correct set.

\# Think about when things maybe read or maybe write

\section{Reordering Program Statements}

\subsection{Method}

Even though we are discussing it, grouping is not a great issue as layout obfuscation is not our primary concern. We want now to learn how
to reorder statements without breaking the program. This means we should be able to receive an ordered list of AST nodes and calculate somehow
the other permutations of statments which have the same visible output. Once we have a method for this we can obfuscate or deobfuscate by our choice
of permutation. An obfuscator would value orderings holding less information and a deobfuscator would favour those with more. How we decide which
orderings hold the information will a a bridge to cross later on.

Our approach to finding possible statement orderings will be for each node in the list to build a list of every other node in the list that that
particular node is dependant on. More simply, if the effect of statement A could change the effect of statement B, then B depends on A. For internal
data such as variable assignments this means if A changes a memory region R that B accesses at some point in the future, then B depends on A.

\subsection{Looking for dependencies}

In calculating dependencies, we will start with the simplest possible list of statements. From here we will expand our reasoning slowly until
we can analyse a substantial portion of the Python language.

\subsubsection{Simplest Case}

Our simplest case would be one where none of the instructions depend on each other. We will ignore external output throughout as we already know
all we need to do is keep any such functions in order. We allow only assignments here as possible statements, and those that bind constants to
variables. The assignments should have no effect other than to add the constant into a new region of memory and assign the given name to
reference this location. This should model a Python program if all identifiers on the left hand side of the assignment are simple names.
(We also ignore the changes Python will make to certain data such as that returned by the \texttt{locals()} built-in function).

An example:

\begin{lstlisting}
a = 1
b = 2
c = []
c = 3
\end{lstlisting}

Here the instructions could move to any order. None of the assignments rely on the previous assignments of any other value to any other name. It's
worth looking at the two assignments of the same variable, \texttt{c}. If we reversed the order of these two statements we would finish the
statement list with either the local dictionary \texttt{\{a: 1, b: 2, c: []\}} or \texttt{\{a: 1, b: 2, c: 3\}}.

In fact this difference does not matter. This change is internal to the program and so not visible to an observer. As we have not specified
any future dependency on \texttt{c}, we can assume that nothing that can affect the observable behaviour of the program will use the value
of \texttt{c} - meaning the program remains correct.

\subsubsection{Slightly Harder}

Now we allow assignments of values that can either be constant or other local names. We will restrict all of our names to local scope as
we have not yet introduced the concept of scope to our model.

An example:

\begin{lstlisting}
a = 1
b = a
c = a
c = b
\end{lstlisting}

Now we can easily go through line by line and look at which names appear on the right of the assignment to gather a list of dependencies.

TODO - CONTINUE FROM HERE!-------------------------------------------------------------

Problem! What if we added one line to make:

\begin{lstlisting}
a = 1
b = a
d = e
c = a
c = b
\end{lstlisting}

Now we have a line that will cause error. 

\section{Opaque Predicates}

The second obfuscation technique we will implement will insert branches into the code that depend on
some opaque predicate \cite{taxobftrans}. This is a predicate that we as the obfuscator know the value of,
but an analyser will find it difficult to evaluate. The branches we will insert will jump either to our valid
program code, or another obfuscated version of our code or even some cleverly disguised garbage. The purpose of doing so being
that the increased number of branches in the program will confuse both a human reader and a programatic analyser.

To reverse the transformation, dead code analysis will be used. This means the analyser will seek to remove
branches where we know a program will always take a certain path. The success of this analysis
depends on both the strength of the analysis and the strength of the opaque predicate.

\section{Relocate or Implement me - TODO}

Both of the described transformations and their analysis will require careful testing. So far,
this has been done using Python's doc test library to write unit tests inside the docstrings
in each piece of code. It is likely a higher level approach will be more suitable later in
the project, testing a program's validity before and after obfuscations as well as the validity
of the analysis.

\section{Evaluation}

\subsection{How to Evaluate}

Evaluation of the final product will be difficult, this is mainly due to the lack of similar tools.
Comparison to other Python obfuscators is not viable as only the pyobfuscate tool is available to
use. While this does perform obfuscation on Python source, it does not operate on the same version of
the source or perform the same type of obfuscation. In fact most available obfuscators will deal with
layout transformation rather than control flow.

There are a few obfuscators that will deal with control flow obfuscation in other languages, for example
the Kava tool for Java is discussed in detail elsewhere \cite{taxobftrans}. We could attempt to compare these
tools with our own, however they tend to be written for much less flexible or dynamic languages than
Python (and hence are easier analysable). Indeed, it is one of the intentions of this project to assess the
extra difficulty a dynamic language such as Python adds when writing these tools, but to compare the final
product to one of these existing tools from such a different base would be misleading.

As well as comparing to similar tools, we have the option to assess an obfuscator's merit based on
its ability to create output that withstands reverse engineering or analysis. In fact this should be
a much better analysis of the product as we can say if the software is good at what it is designed to do, rather
than whether it is just better at performing the same function as another tool.

To aid in this form of evaluation we will need tools to analyse the obfuscated source code. There are plenty
mentioned in the background section, however these tend to be designed to optimise code or detect possible bugs and so
are not ideal.

The solution proposed for this project is to write tools for analysis along side the tools for obfuscation.
By doing this we can really target the analysis at detecting and reversing the particular transformations
that our obfuscator implements. Hopefully this should not create a great deal of extra work as the analysis
needed for reversing a transformation is often likely to be the same form of analysis performed to make that
transformation in the first place, so the work is already done.

Of course this is not a perfect solution. Ideally analytic tools would be written by someone else without
knowledge of the specific obfuscator. This would move closer to representing a reverse-engineer or analytic
tool in the real world, as hopefully neither would be able to ascertain the software used to perform the obfuscation
or the specific techniques used on specific code locations. In theory this should give the analytic tool an
advantage, lowering our expected potency for the obfuscator.

Another problem caused by the same author writing both sides of a competing set of software is that the competition
may not be so fierce as in the real world. Any design oversights in one half of the software will likely show up
in the other, rather than being exploited as would hopefully be the case with two free thinking developers. As a real
example, the Java decompiler Mocha will crash if it finds extra statements after a function has returned
\cite{hosemocha}[p4]. The bytecode obfuscator HoseMocha uses this to prevent Mocha from decompiling programs. Had these two
pieces of software been written by the same developer, one whose decompiler doesn't check for the unlikely case described,
then it is probable that this case would never have been thought of for the obfuscator - and HoseMocha would never have
been created.

We do not have the option of contracting a separate developer to help with this evaluation. The best solution to mitigate the effect
of this is to compare the development of the analysis tool to the available analytic tools discussed in the background section. Although
these have different primary functions, at least some functionality will overlap and allow a loose comparison. From this it
should be possible to determine if our analysis tool is sub-par or better than average and to judge the obfuscator
accordingly.

\subsection{What to Evaluate}

We have looked at how we can evaluate the software, and we know the tools we will use to do so. What we don't know yet is
the specific criteria that we will be evaluating. 

As we already know, the aim of obfuscation is to confuse a human reader or a programmatic analysis. In measuring the effectiveness
of a particular obfuscation we are actually attempting to quantify the additional confusion caused by it. This is not an easy task
as confusion is very subjective to a particular viewer or program.

Previous attempts to do this tend to use program complexity metrics to measure the impact of a particular obfuscation. Many fail to
actually quantify transformation benefit, preferring instead to give a rough estimation, although recently there have been improvements in
this area \cite{obfquant}.

Although these metrics can be used during the obfuscating and analysis, they are unlikely to help with evaluation as we are
not proposing any new transformations. Instead we are comparing the same transformation in an environment with looser rules than usual.
In this case, complexity metrics will be very similar if not the same and so not helpful to us.

A much more helpful approach will be to discuss freely (and using the metrics) the challenges faced in implementing these obfuscations
in Python as well as breaking them. This can be compared to other languages and implementations. Finally the comparative performances
of the obfuscator and analyser will be assessed on some example programs.

\section{Conclusions and Future Work}

TODO - this section, aaah

\bibliographystyle{plain}
\bibliography{final}

\end{document}
